{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.layers import GlobalAveragePooling2D, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB * 2 of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 2)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "import uuid\n",
    "arr_guid = [str(uuid.uuid4()) for i in range(BATCH_SIZE*NUM_CLASSES)] \n",
    "\n",
    "# importing copy module \n",
    "import copy \n",
    "\n",
    "classes = []\n",
    "c_obj = {\n",
    "    'meta' : {},\n",
    "    'blocks': []\n",
    "}\n",
    "\n",
    "cifar_class_names = ['Airplane','Automobile','Bird','Cat','Deer','Dog','Frog','Horse','Ship','Truck']\n",
    "colors = [\"#1f77b4\",\"#ff7f0e\",\"#2ca02c\",\"#d62728\",\"#9467bd\",\"#8c564b\",\"#e377c2\",\"#7f7f7f\",\"#bcbd22\",\"#17becf\"]\n",
    "\n",
    "# using deepcopy for deepcopy \n",
    "for idx,i in enumerate(zip(cifar_class_names,colors)):\n",
    "    tmpX = copy.deepcopy(c_obj)\n",
    "    tmpX['meta']['name'] = str(i[0])\n",
    "    tmpX['meta']['color'] = str(i[1])\n",
    "    classes.append(tmpX)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CIFAR\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instantiate the model \n",
    "# https://keras.io/api/applications/resnet/#resnet-and-resnetv2\n",
    "# TODO : upscaling 32x32 \n",
    "#        When setting `include_top=True` and loading `imagenet` weights, `input_shape` should be (224, 224, 3).\n",
    "\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "\n",
    "base_model = {}\n",
    "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "#base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape, pooling='avg')\n",
    "\n",
    "print(base_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Block_layers = [37,79,141,173]\n",
    "for idx in range(len(Block_layers)):\n",
    "    print(idx, base_model.get_layer(index = Block_layers[idx]).name)\n",
    "    #print(idx, base_model.get_layer(index = idx).__class__.__name__)\n",
    "    \n",
    "    #['BatchNormalization','ZeroPadding2D','max_pooling2d_1']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the batches here (ideally we want BATCH_SIZE samples for all NUM_CLASSES classes)\n",
    "all_classes = np.unique(y_test)\n",
    "\n",
    "# total batch images:\n",
    "mini_batches = []\n",
    "\n",
    "mini_y = []\n",
    "\n",
    "for c_i in all_classes:\n",
    "    result = np.where(y_test == c_i)\n",
    "    batch = result[0][:BATCH_SIZE].astype(int)\n",
    "    \n",
    "    mini_y = np.concatenate((mini_y,batch))\n",
    "    \n",
    "    for idx in batch:\n",
    "        mini_batches.append(x_test[idx])\n",
    "        \n",
    "        \n",
    "mini_batches = np.stack(mini_batches)\n",
    "print(mini_batches.shape)\n",
    "\n",
    "mini_y = mini_y.astype(int)\n",
    "print(mini_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batches = mini_batches/255.0\n",
    "np.min(mini_batches),np.max(mini_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA from sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "tmp_layers_class_0 = []\n",
    "tmp_layers_class_1 = []\n",
    "tmp_layers_class_2 = []\n",
    "tmp_layers_class_3 = []\n",
    "tmp_layers_class_4 = []\n",
    "tmp_layers_class_5 = []\n",
    "tmp_layers_class_6 = []\n",
    "tmp_layers_class_7 = []\n",
    "tmp_layers_class_8 = []\n",
    "tmp_layers_class_9 = []\n",
    "\n",
    "for idx in range(len(base_model.layers)):\n",
    "    avoid_layers = ['BatchNormalization','ZeroPadding2D','max_pooling2d_1']\n",
    "    layer_name = base_model.get_layer(index = idx).name\n",
    "    layer_type = base_model.get_layer(index = idx).__class__.__name__\n",
    "    \n",
    "    if layer_type not in avoid_layers:\n",
    "\n",
    "        x = base_model.layers[idx].output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        model_cut = Model(inputs = base_model.input, outputs = x)\n",
    "\n",
    "        pred = model_cut.predict(mini_batches)\n",
    "\n",
    "\n",
    "        # Instantiate PCA\n",
    "        pca = PCA(n_components=3)\n",
    "        X = pca.fit_transform(pred)\n",
    "\n",
    "        print(X.shape)\n",
    "        X = X[:,:2]  #just keep the two columns for visualization\n",
    "        \"\"\"\n",
    "        # Create a new dataset from principal components \n",
    "        df = pd.DataFrame(data = X, columns = ['PC1', 'PC2'])\n",
    "\n",
    "        target = pd.Series(np.array(y_test[mini_y]).flatten(), name='y')\n",
    "\n",
    "        result_df = pd.concat([df, target], axis=1)\n",
    "\n",
    "\n",
    "        # Visualize Principal Components with a scatter plot\n",
    "        fig = plt.figure(figsize = (8,6))\n",
    "        ax = fig.add_subplot(1,1,1) \n",
    "        ax.set_xlabel('First Principal Component ', fontsize = 15)\n",
    "        ax.set_ylabel('Second Principal Component ', fontsize = 15)\n",
    "        ax.set_title('Principal Component Analysis (2PCs)', fontsize = 20)\n",
    "\n",
    "        targets = np.unique(y_test)\n",
    "        colors = [\"#1f77b4\",\"#ff7f0e\",\"#2ca02c\",\"#d62728\",\"#9467bd\",\"#8c564b\",\"#e377c2\",\"#7f7f7f\",\"#bcbd22\",\"#17becf\"]\n",
    "        for target, color in zip(targets, colors):\n",
    "            indicesToKeep = result_df.loc[result_df['y'] == target]\n",
    "            ax.scatter(indicesToKeep['PC1'], \n",
    "                       indicesToKeep['PC2'], \n",
    "                       c = color, \n",
    "                       s = 50)\n",
    "        ax.legend(targets)\n",
    "        ax.grid()\n",
    "        \"\"\"\n",
    "\n",
    "        tmp_layers = []\n",
    "        for idx, i in enumerate(X):\n",
    "            if idx == 100:\n",
    "                l_obj = {\n",
    "                    \"name\" : layer_name,\n",
    "                    \"layers\" : tmp_layers\n",
    "                }\n",
    "                tmp_layers_class_0.append(l_obj)\n",
    "                \n",
    "                tmp_layers = []\n",
    "            elif idx == 200:\n",
    "                l_obj = {\n",
    "                    \"name\" : layer_name,\n",
    "                    \"layers\" : tmp_layers\n",
    "                }\n",
    "                tmp_layers_class_1.append(l_obj)\n",
    "                tmp_layers = []\n",
    "            elif idx == 300:\n",
    "                l_obj = {\n",
    "                    \"name\" : layer_name,\n",
    "                    \"layers\" : tmp_layers\n",
    "                }\n",
    "                tmp_layers_class_2.append(l_obj)\n",
    "                tmp_layers = []\n",
    "            elif idx == 400:\n",
    "                l_obj = {\n",
    "                    \"name\" : layer_name,\n",
    "                    \"layers\" : tmp_layers\n",
    "                }\n",
    "                tmp_layers_class_3.append(l_obj)\n",
    "                tmp_layers = []\n",
    "            elif idx == 500:\n",
    "                l_obj = {\n",
    "                    \"name\" : layer_name,\n",
    "                    \"layers\" : tmp_layers\n",
    "                }\n",
    "                tmp_layers_class_4.append(l_obj)\n",
    "                tmp_layers = []\n",
    "            elif idx == 600:\n",
    "                l_obj = {\n",
    "                    \"name\" : layer_name,\n",
    "                    \"layers\" : tmp_layers\n",
    "                }\n",
    "                tmp_layers_class_5.append(l_obj)\n",
    "                tmp_layers = []\n",
    "            elif idx == 700:\n",
    "                l_obj = {\n",
    "                    \"name\" : layer_name,\n",
    "                    \"layers\" : tmp_layers\n",
    "                }\n",
    "                tmp_layers_class_6.append(l_obj)\n",
    "                tmp_layers = []\n",
    "            elif idx == 800:\n",
    "                l_obj = {\n",
    "                    \"name\" : layer_name,\n",
    "                    \"layers\" : tmp_layers\n",
    "                }\n",
    "                tmp_layers_class_7.append(l_obj)\n",
    "                tmp_layers = []\n",
    "            elif idx == 900:\n",
    "                l_obj = {\n",
    "                    \"name\" : layer_name,\n",
    "                    \"layers\" : tmp_layers\n",
    "                }\n",
    "                tmp_layers_class_8.append(l_obj)\n",
    "                tmp_layers = []\n",
    "\n",
    "\n",
    "            point = {\n",
    "                'x': str(i[0]),\n",
    "                'y': str(i[1]),\n",
    "                'pred': str(y_test[mini_y[idx]][0]),\n",
    "                'actual': str(y_test[mini_y[idx]][0]),\n",
    "                'guid': arr_guid[idx]\n",
    "            }            \n",
    "            tmp_layers.append(point)\n",
    "\n",
    "            if idx == 999:\n",
    "                l_obj = {\n",
    "                    \"name\" : layer_name,\n",
    "                    \"layers\" : tmp_layers\n",
    "                }\n",
    "                tmp_layers_class_9.append(l_obj)\n",
    "                tmp_layers = []\n",
    "        \n",
    "\n",
    "#for idx in range(len(base_model.layers)):\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "for f in os.listdir('..\\\\..\\\\data\\\\v1\\\\images'):\n",
    "    os.remove(os.path.join('..\\\\..\\\\data\\\\v1\\\\images', f))\n",
    "\n",
    "for idx, i in enumerate(arr_guid):\n",
    "    x_index = mini_y[idx]\n",
    "    image = Image.fromarray(x_test[x_index])\n",
    "    image.save('..\\\\..\\\\data\\\\v1\\\\images\\\\'+i+'.png')\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.path.abspath('..\\\\data\\\\v1\\\\images\\\\'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "result = {}\n",
    "result[\"n_layers\"] = str(len(add_layers))\n",
    "result[\"classes\"] = classes\n",
    "\n",
    "with open('..\\\\..\\\\data\\\\v1\\\\block_resnet50.json', 'w') as outfile:\n",
    "    json.dump(result, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Import PCA from sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Instantiate PCA\n",
    "pca = PCA(n_components=3)\n",
    "X = pca.fit_transform(pred)\n",
    "\n",
    "print(X.shape)\n",
    "X = X[:,:2]  #just keep the two columns for visualization\n",
    "\n",
    "# Create a new dataset from principal components \n",
    "df = pd.DataFrame(data = X, columns = ['PC1', 'PC2'])\n",
    "\n",
    "target = pd.Series(np.array(y_test[0:1000]).flatten(), name='y')\n",
    "\n",
    "result_df = pd.concat([df, target], axis=1)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize Principal Components with a scatter plot\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "fig = plt.figure(figsize = (8,6))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('First Principal Component ', fontsize = 15)\n",
    "ax.set_ylabel('Second Principal Component ', fontsize = 15)\n",
    "ax.set_title('Principal Component Analysis (2PCs)', fontsize = 20)\n",
    "\n",
    "targets = np.unique(y_test)\n",
    "#targets = [4,1]\n",
    "colors = [\"#1f77b4\",\"#ff7f0e\",\"#2ca02c\",\"#d62728\",\"#9467bd\",\"#8c564b\",\"#e377c2\",\"#7f7f7f\",\"#bcbd22\",\"#17becf\"]\n",
    "#colors = [\"#1f77b4\",\"#ff7f0e\"]\n",
    "for target, color in zip(targets, colors):\n",
    "    indicesToKeep = result_df.loc[result_df['y'] == target]\n",
    "    ax.scatter(indicesToKeep['PC1'], \n",
    "               indicesToKeep['PC2'], \n",
    "               c = color, \n",
    "               s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate method\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalAveragePooling2D\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "model = Model(inputs = base_model.input, outputs = x)\n",
    "\n",
    "#model = keras.Sequential(\n",
    "#    [\n",
    "#       base_model,\n",
    "#        keras.layers.Flatten(),\n",
    "#        keras.layers.Dense(1024, activation=\"relu\"),\n",
    "#        keras.layers.Dense(10, activation=\"softmax\"),\n",
    "#    ]\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = np.expand_dims(x_test[0], axis=0)\n",
    "image = x_test[0:1000]\n",
    "pred = model_cut.predict(image)\n",
    "\n",
    "pred.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu-kernel",
   "language": "python",
   "name": "keras-gpu-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
