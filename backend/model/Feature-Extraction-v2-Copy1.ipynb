{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.layers import GlobalAveragePooling2D, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB * 2 of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 2)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 5\n",
    "NUM_EPOCHS = 10\n",
    "use_data_aug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# img_arr is of shape (n, h, w, c)\n",
    "def resize_image_arr(img_arr):\n",
    "    x_resized_list = []\n",
    "    for i in tqdm(range(img_arr.shape[0])):\n",
    "        img = img_arr[0]\n",
    "        resized_img = resize(img, (224, 224))\n",
    "        x_resized_list.append(resized_img)\n",
    "    return np.stack(x_resized_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x_df= x_test.reshape(10000, x_test.shape[1] * x_test.shape[2] *x_test.shape[3] )\n",
    "\n",
    "imgs = pd.DataFrame() \n",
    "df_labels = pd.DataFrame() \n",
    "\n",
    "imgs = imgs.append(pd.DataFrame(x_df)) \n",
    "df_labels = df_labels.append(pd.DataFrame(y_test)) \n",
    "imgs['labels'] = df_labels \n",
    "\n",
    "imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [01:35<00:00, 104.19it/s]\n"
     ]
    }
   ],
   "source": [
    "x_test = resize_image_arr(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = x_test/255.0\n",
    "np.min(x_test),np.max(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataframe: (10000, 3073)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "x_test_flat = x_test.reshape(-1,3072)\n",
    "feat_cols = ['pixel'+str(i) for i in range(x_test_flat.shape[1])]\n",
    "df_cifar = pd.DataFrame(x_test_flat,columns=feat_cols)\n",
    "df_cifar['label'] = y_test\n",
    "print('Size of the dataframe: {}'.format(df_cifar.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>principal component 1</th>\n",
       "      <th>principal component 2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.694406</td>\n",
       "      <td>0.871710</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.688801</td>\n",
       "      <td>-9.689979</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.423473</td>\n",
       "      <td>-8.887094</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.755905</td>\n",
       "      <td>-3.889165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.463532</td>\n",
       "      <td>4.298671</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   principal component 1  principal component 2  y\n",
       "0              -3.694406               0.871710  3\n",
       "1               9.688801              -9.689979  8\n",
       "2               4.423473              -8.887094  8\n",
       "3               7.755905              -3.889165  0\n",
       "4              -5.463532               4.298671  6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import PCA from sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_cifar = PCA(n_components=2)\n",
    "principalComponents_cifar = pca_cifar.fit_transform(df_cifar.iloc[:,:-1])\n",
    "principal_cifar_Df = pd.DataFrame(data = principalComponents_cifar\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "principal_cifar_Df['y'] = y_test\n",
    "principal_cifar_Df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "#y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# Normalize the data\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50(include_top=False, weights= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = imgs.loc[imgs['labels'] == i][:100]\n",
    "\n",
    "abc = img_idx.iloc[:, :3072]\n",
    "\n",
    "for idx, row in abc.iterrows():\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize, scale, MinMaxScaler\n",
    "\n",
    "(x_train_SAMP, y_train_SAMP), (x_test_SAMP, y_test_SAMP) = cifar10.load_data()\n",
    "\n",
    "x_train_SAMP =[]\n",
    "y_train_SAMP =[]\n",
    "\n",
    "result = {}\n",
    "LAYER_COUNTER = 0\n",
    "for idx in range(len(base_model.layers)):\n",
    "\n",
    "    \n",
    "    if base_model.get_layer(index = idx).__class__.__name__ == 'Conv2D':\n",
    "        # PREPARE FOR JSON \n",
    "        result['dataset_'+str(LAYER_COUNTER)]= []\n",
    "    \n",
    "        for i in range(0,10):\n",
    "            \n",
    "            \n",
    "            img_idx = imgs.loc[imgs['labels'] == i][:100]\n",
    "            \n",
    "            temp_img_arr = []\n",
    "            for index, row in img_idx.iloc[:, :3072].iterrows():\n",
    "                #print(x_test_SAMP[index].shape)\n",
    "                #Xtest = np.reshape(data[b\"data\"][3], (-1, 3, 32, 32))\n",
    "                #Xtest = np.transpose(Xtest, (0,2,3,1))\n",
    "\n",
    "                #x1 = np.reshape(np.asarray(row),(-1,32,32,3))\n",
    "                #x1 = np.transpose(x1, (0,2,3,1))\n",
    "                temp_img_arr.append(x_test[index])\n",
    "            \n",
    "            #print(temp_img_arr)\n",
    "            #temp_img_arr = np.array(temp_img_arr).reshape(-1,32,32,3)\n",
    "            temp_img_arr = np.stack(temp_img_arr,axis=0)\n",
    "            print(temp_img_arr.shape)\n",
    "            \n",
    "            x_batch = resize_image_arr(temp_img_arr)\n",
    "            print(x_batch.shape)\n",
    "\n",
    "            x_batch = x_batch.astype('float32')\n",
    "            x_batch /= 255\n",
    "            \n",
    "            \n",
    "            layerName = base_model.get_layer(index = idx).name\n",
    "            print(idx, layerName)\n",
    "\n",
    "            model_cut = Model(inputs=base_model.inputs, output=base_model.layers[idx].output)\n",
    "            preds = model_cut.predict(x_batch)\n",
    "            print('preds',preds.shape)\n",
    "\n",
    "            preds_flat = preds.reshape(preds.shape[0],preds.shape[1] * preds.shape[2] * preds.shape[3])\n",
    "            print(preds_flat.shape)\n",
    "\n",
    "            import pandas as pd \n",
    "\n",
    "            X = preds_flat\n",
    "            # Standardize the features\n",
    "            #X = StandardScaler().fit_transform(preds_flat)\n",
    "            #X = normalize(preds_flat)\n",
    "            #X = scale(preds_flat)\n",
    "\n",
    "            #mm_scaler = MinMaxScaler()\n",
    "            #X = mm_scaler.fit_transform(preds_flat)\n",
    "            #mm_scaler.transform(X)\n",
    "\n",
    "            # Preview X\n",
    "            pd.DataFrame(data=X).head()\n",
    "\n",
    "            # Import PCA from sklearn\n",
    "            from sklearn.decomposition import PCA\n",
    "\n",
    "            # Instantiate PCA\n",
    "            pca = PCA(n_components=2)\n",
    "\n",
    "            # Fit PCA to features\n",
    "            principalComponents = pca.fit_transform(X)\n",
    "            \n",
    "            CLASS_PCA_RES = []\n",
    "            for ipc in range(principalComponents.shape[0]):\n",
    "                arr_classPt = {\n",
    "                    \"uid\" : \"\",\n",
    "                    \"filename\" : \"\",\n",
    "                    \"comp1\": str(principalComponents[ipc][0]),\n",
    "                    \"comp2\": str(principalComponents[ipc][1])\n",
    "                }\n",
    "                CLASS_PCA_RES.append(arr_classPt)\n",
    "                \n",
    "            result[\"dataset_\"+str(LAYER_COUNTER)].append(CLASS_PCA_RES)\n",
    "\n",
    "            # Create a new dataset from principal components \n",
    "            df = pd.DataFrame(data = principalComponents, \n",
    "                              columns = ['PC1', 'PC2'])\n",
    "\n",
    "            target = pd.Series(y_test_SAMP[:100].reshape(100,), name='target')\n",
    "\n",
    "            result_df = pd.concat([df, target], axis=1)\n",
    "            print(result_df)\n",
    "        \n",
    "        LAYER_COUNTER+= 1\n",
    "    ##if idx > 10:\n",
    "     #   break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[:10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize, scale, MinMaxScaler\n",
    "\n",
    "(x_train_SAMP, y_train_SAMP), (x_test_SAMP, y_test_SAMP) = cifar10.load_data()\n",
    "\n",
    "x_train_SAMP =[]\n",
    "y_train_SAMP =[]\n",
    "\n",
    "\n",
    "\n",
    "for idx in range(len(base_model.layers)):\n",
    "    if base_model.get_layer(index = idx).__class__.__name__ == 'Conv2D':\n",
    "        for i in range(0,10):\n",
    "            x_batch = x_test[:100]\n",
    "            \n",
    "           \n",
    "            \n",
    "            layerName = base_model.get_layer(index = idx).name\n",
    "            print(idx, layerName)\n",
    "\n",
    "            model_cut = Model(inputs=base_model.inputs, output=base_model.layers[idx].output)\n",
    "            preds = model_cut.predict(x_batch)\n",
    "            print('preds',preds.shape)\n",
    "\n",
    "            preds_flat = preds.reshape(preds.shape[0],preds.shape[1] * preds.shape[2] * preds.shape[3])\n",
    "            print(preds_flat.shape)\n",
    "\n",
    "            import pandas as pd \n",
    "\n",
    "            X = preds_flat\n",
    "            # Standardize the features\n",
    "            #X = StandardScaler().fit_transform(preds_flat)\n",
    "            #X = normalize(preds_flat)\n",
    "            #X = scale(preds_flat)\n",
    "\n",
    "            #mm_scaler = MinMaxScaler()\n",
    "            #X = mm_scaler.fit_transform(preds_flat)\n",
    "            #mm_scaler.transform(X)\n",
    "\n",
    "            # Preview X\n",
    "            pd.DataFrame(data=X).head()\n",
    "\n",
    "            # Import PCA from sklearn\n",
    "            from sklearn.decomposition import PCA\n",
    "\n",
    "            # Instantiate PCA\n",
    "            pca = PCA(n_components=2)\n",
    "\n",
    "            # Fit PCA to features\n",
    "            principalComponents = pca.fit_transform(X)\n",
    "\n",
    "            # Create a new dataset from principal components \n",
    "            df = pd.DataFrame(data = principalComponents, \n",
    "                              columns = ['PC1', 'PC2'])\n",
    "\n",
    "            target = pd.Series(y_test_SAMP[:10].reshape(10,), name='target')\n",
    "\n",
    "            result_df = pd.concat([df, target], axis=1)\n",
    "            print(result_df)\n",
    "        \n",
    "    if idx > 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "# Standardize the features\n",
    "#X = StandardScaler().fit_transform(preds_flat)\n",
    "\n",
    "# Preview X\n",
    "pd.DataFrame(data=preds_flat).head()\n",
    "\n",
    "# Import PCA from sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Instantiate PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA to features\n",
    "principalComponents = pca.fit_transform(preds_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Principal Components with a scatter plot\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "fig = plt.figure(figsize = (12,10))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('First Principal Component ', fontsize = 15)\n",
    "ax.set_ylabel('Second Principal Component ', fontsize = 15)\n",
    "ax.set_title('Principal Component Analysis (2PCs) for Iris Dataset', fontsize = 20)\n",
    "\n",
    "targets = [3, 1, 8, 0 ,6]\n",
    "colors = ['r', 'g', 'b', 'c', 'y']\n",
    "for target, color in zip(targets, colors):\n",
    "    indicesToKeep = y_test_SAMP[:10].reshape(10,) == target\n",
    "    ax.scatter(result_df.loc[indicesToKeep, 'PC1'], \n",
    "               result_df.loc[indicesToKeep, 'PC2'], \n",
    "               c = color, \n",
    "               s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "result[\"n_layers\"] = str(LAYER_COUNTER)\n",
    "\n",
    "with open('sample_json3.json', 'w') as outfile:\n",
    "    json.dump(result, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(x_test),np.max(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu-kernel",
   "language": "python",
   "name": "keras-gpu-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
